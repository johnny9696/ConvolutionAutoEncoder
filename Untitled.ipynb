{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ef959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StackedAE import Convolution_Auto_Encoder as CAE\n",
    "from StackedAE import Convolution_AE_Classification as CAC\n",
    "import torch\n",
    "import torch.nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(\"\"))))\n",
    "import commons\n",
    "import utils\n",
    "import audio_processing as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09bb623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/caijb/data_drive/autoencoder/log/kernel5/G_203.pth\n",
      "INFO:root:Loaded checkpoint '/media/caijb/data_drive/autoencoder/log/kernel5/G_203.pth' (iteration 203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Convolution_Auto_Encoder(\n",
       "   (encoder): Encoder(\n",
       "     (conv2d_layer_1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (conv2d_layer_2): Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (relu): ReLU()\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (Tconv2d_layer1): ConvTranspose2d(5, 3, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (Tconv2d_layer2): ConvTranspose2d(3, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (relu): ReLU()\n",
       "   )\n",
       " ),\n",
       " None,\n",
       " 0.0001,\n",
       " 203)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model load from save path\n",
    "config_path =  \"/media/caijb/data_drive/autoencoder/log/kernel5/config.json\"\n",
    "\n",
    "#load paramters\n",
    "with open(config_path, 'r') as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "hps = utils.HParams(**config)\n",
    "\n",
    "#load model & load saved model weight\n",
    "checkpoint_path = utils.latest_checkpoint_path(hps.train.model_dir)\n",
    "\n",
    "model = CAE(encoder_dim=hps.model.encoder_dim, hidden_1dim=hps.model.hidden_dim1,\n",
    "    hidden_2dim=hps.model.hidden_dim2, kernel=hps.model.kernel)\n",
    "\n",
    "utils.load_checkpoint(checkpoint_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3387f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.conv2d_layer_1.weight', 'encoder.conv2d_layer_1.bias', 'encoder.conv2d_layer_2.weight', 'encoder.conv2d_layer_2.bias', 'decoder.Tconv2d_layer1.weight', 'decoder.Tconv2d_layer1.bias', 'decoder.Tconv2d_layer2.weight', 'decoder.Tconv2d_layer2.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964177bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor= torch.randn(1,80,430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15befa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 72, 422])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=model.get_vector(tensor)\n",
    "encoder.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67fe8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = CAC(encoder_dim=hps.model.encoder_dim, hidden_1dim=hps.model.hidden_dim1,\n",
    "    hidden_2dim=hps.model.hidden_dim2, kernel=hps.model.kernel, hps = hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd573f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.conv2d_layer_1.weight', 'encoder.conv2d_layer_1.bias', 'encoder.conv2d_layer_2.weight', 'encoder.conv2d_layer_2.bias', 'classification.conv1x1.weight', 'classification.conv1x1.bias', 'classification.linear1.weight', 'classification.linear1.bias', 'classification.linear2.weight', 'classification.linear2.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa7f9d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0604,  0.0382, -0.1735,  0.1476,  0.0644],\n",
       "          [ 0.1883,  0.0865,  0.0510, -0.0497,  0.1990],\n",
       "          [-0.0685,  0.0044, -0.1877, -0.0646, -0.1982],\n",
       "          [ 0.1326, -0.0809,  0.1613, -0.1155,  0.1867],\n",
       "          [ 0.1602,  0.0622,  0.0182,  0.0124, -0.1064]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1297,  0.0754, -0.0991, -0.1895, -0.0160],\n",
       "          [-0.0892,  0.0996, -0.1496, -0.0600,  0.0604],\n",
       "          [ 0.1896,  0.1613, -0.0324,  0.1505, -0.1681],\n",
       "          [-0.1152, -0.1162, -0.1452, -0.1286, -0.1966],\n",
       "          [-0.1595, -0.0805,  0.0193, -0.0246,  0.0146]]],\n",
       "\n",
       "\n",
       "        [[[-0.1892,  0.1324,  0.0543, -0.1169,  0.0229],\n",
       "          [-0.0313,  0.1014, -0.1026,  0.1828,  0.1123],\n",
       "          [ 0.1641, -0.1198, -0.1945,  0.1965,  0.0252],\n",
       "          [ 0.0557, -0.0702,  0.1584,  0.0303, -0.1357],\n",
       "          [-0.0344, -0.0298,  0.1241, -0.0257, -0.0713]]]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46da261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0257, -0.1003,  0.3286,  0.4681, -0.1894],\n",
       "          [-0.1451,  0.1371, -0.1216,  0.4228, -0.1207],\n",
       "          [ 0.0255, -0.0569, -0.0945, -0.0432, -0.1239],\n",
       "          [ 0.1142, -0.1354,  0.0706,  0.0328,  0.0506],\n",
       "          [ 0.1053,  0.2175,  0.0425, -0.0424, -0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0859,  0.1653,  0.2484,  0.2368, -0.0686],\n",
       "          [ 0.0818,  0.1169, -0.1984, -0.1115, -0.0978],\n",
       "          [-0.0162,  0.0448,  0.1337,  0.0938,  0.2012],\n",
       "          [-0.0523,  0.0512, -0.1359, -0.1301, -0.0707],\n",
       "          [-0.0996, -0.0348,  0.1467,  0.0944, -0.0671]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376, -0.0479, -0.1037, -0.0086,  0.0558],\n",
       "          [ 0.0264,  0.0793,  0.1925, -0.3262, -0.0777],\n",
       "          [ 0.0345,  0.1169, -0.2020,  0.1778,  0.2321],\n",
       "          [ 0.2132, -0.0024,  0.0219, -0.1180, -0.0346],\n",
       "          [ 0.2287,  0.2257, -0.0916,  0.0153, -0.0708]]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d1ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.encoder=model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84cff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0257, -0.1003,  0.3286,  0.4681, -0.1894],\n",
       "          [-0.1451,  0.1371, -0.1216,  0.4228, -0.1207],\n",
       "          [ 0.0255, -0.0569, -0.0945, -0.0432, -0.1239],\n",
       "          [ 0.1142, -0.1354,  0.0706,  0.0328,  0.0506],\n",
       "          [ 0.1053,  0.2175,  0.0425, -0.0424, -0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0859,  0.1653,  0.2484,  0.2368, -0.0686],\n",
       "          [ 0.0818,  0.1169, -0.1984, -0.1115, -0.0978],\n",
       "          [-0.0162,  0.0448,  0.1337,  0.0938,  0.2012],\n",
       "          [-0.0523,  0.0512, -0.1359, -0.1301, -0.0707],\n",
       "          [-0.0996, -0.0348,  0.1467,  0.0944, -0.0671]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376, -0.0479, -0.1037, -0.0086,  0.0558],\n",
       "          [ 0.0264,  0.0793,  0.1925, -0.3262, -0.0777],\n",
       "          [ 0.0345,  0.1169, -0.2020,  0.1778,  0.2321],\n",
       "          [ 0.2132, -0.0024,  0.0219, -0.1180, -0.0346],\n",
       "          [ 0.2287,  0.2257, -0.0916,  0.0153, -0.0708]]]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.encoder.conv2d_layer_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed7b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
